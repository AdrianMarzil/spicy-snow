{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions to search and download Sentinel-1 images for specific geometries and dates\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from os.path import basename, exists, expanduser, join\n",
    "import shutil\n",
    "import asf_search as asf\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray as rxa\n",
    "from rioxarray.merge import merge_arrays\n",
    "import shapely.geometry\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "import hyp3_sdk as sdk\n",
    "from hyp3_sdk.exceptions import AuthenticationError\n",
    "\n",
    "sys.path.append(expanduser('~/Documents/VS CODE/spicy-snow'))\n",
    "from spicy_snow.utils.download import url_download\n",
    "\n",
    "def s1_img_search(area: shapely.geometry.box, dates: (str, str)) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    find dates and url of Sentinel-1 overpasses\n",
    "\n",
    "    Args:\n",
    "    area: Bounding box of desired area to search within\n",
    "    dates: Start and end date to search between\n",
    "\n",
    "    Returns:\n",
    "    granules: Dataframe of Sentinel-1 granule names to download.\n",
    "    \"\"\"\n",
    "    # Error Checking\n",
    "    if len(dates) != 2:\n",
    "        raise TypeError(\"Provide at start and end date in format (YYYY-MM-DD, YYYY_MM_DD)\")\n",
    "    if type(area) != shapely.geometry.polygon.Polygon:\n",
    "        raise TypeError(\"Geometry must be a shapely.geometry.box type\")\n",
    "    if type(dates[0]) != str:\n",
    "        raise TypeError(\"Provide at start and end date in format (YYYY-MM-DD, YYYY_MM_DD)\")\n",
    "    dates = [pd.to_datetime(d) for d in dates]\n",
    "    if dates[1] < dates[0]:\n",
    "        raise ValueError(\"End date is before start date\")\n",
    "    if dates[0].year < 2017 or dates[1].year < 2017:\n",
    "        raise IndexError(\"Dates are prior to Sentinel-1 launch dates\")\n",
    "    if dates[0].date() > date.today() or dates[1].date() > date.today():\n",
    "        raise IndexError(\"Dates are in the future.\")\n",
    "    if area.bounds[3] > 90 or area.bounds[1] < 0 or area.bounds[2] > 180\\\n",
    "        or area.bounds[0] < -180:\n",
    "        raise IndexError(\"Coordinates must be between 0-90N and -180-180\")\n",
    "\n",
    "    # get results from asf_search in date range and geometry\n",
    "    results = asf.geo_search(platform = [asf.PLATFORM.SENTINEL1], intersectsWith = area.wkt,\\\n",
    "        start = dates[0], end = dates[1], processingLevel = asf.PRODUCT_TYPE.GRD_HD)\n",
    "\n",
    "    # check with 0 results.\n",
    "    if len(results) == 0:\n",
    "        raise ValueError(\"No search results found.\")\n",
    "\n",
    "    # create pandas dataframe from json result\n",
    "    results = pd.json_normalize(results.geojson(), record_path = ['features'])\n",
    "\n",
    "    return results\n",
    "\n",
    "def hyp3_pipeline(search_results: pd.DataFrame, job_name, existing_job_name = False) -> sdk.jobs.Batch:\n",
    "    \"\"\"\n",
    "    Start and monitor Hyp3 pipeline for desired Sentinel-1 granules\n",
    "    https://hyp3-docs.asf.alaska.edu/using/sdk_api/\n",
    "\n",
    "    Args:\n",
    "    search_results: Pandas Dataframe of asf_search search results.\n",
    "    job_name: name to give hyp3 batch run\n",
    "    existing_job_name: if you have an existing job that you want to find and reuse [default: False]\n",
    "\n",
    "    Returns:\n",
    "    rtc_jobs: Hyp3 batch object of completed jobs.\n",
    "    \"\"\" \n",
    "    try:\n",
    "        # .netrc\n",
    "        hyp3 = sdk.HyP3()\n",
    "    except AuthenticationError:\n",
    "        # prompt for password\n",
    "        hyp3 = sdk.HyP3(prompt = True)\n",
    "\n",
    "    # if existing job name provided then don't submit and simply watch existing jobs.\n",
    "    while existing_job_name:\n",
    "        rtc_jobs = hyp3.find_jobs(name = existing_job_name)\n",
    "\n",
    "        # if no jobs found go to original search with name.\n",
    "        if len(rtc_jobs) == 0:\n",
    "            break\n",
    "\n",
    "        # if no running jobs then just return succeeded jobs\n",
    "        if len(rtc_jobs.filter_jobs(succeeded = False, failed = False)) == 0:\n",
    "            return rtc_jobs.filter_jobs(succeeded = True)\n",
    "\n",
    "        # otherwise watch running jobs\n",
    "        hyp3.watch(rtc_jobs)\n",
    "\n",
    "        # refresh with new successes and failures\n",
    "        rtc_jobs = hyp3.refresh(rtc_jobs)\n",
    "        \n",
    "        # return successful jobs\n",
    "        return rtc_jobs.filter_jobs(succeeded = True)\n",
    "\n",
    "    # gather granules to submit to the hyp3 pipeline\n",
    "    granules = search_results['properties.sceneName']\n",
    "\n",
    "    # create a new hyp3 batch to hold submitted jobs\n",
    "    rtc_jobs = sdk.Batch()\n",
    "    for g in tqdm(granules, desc = 'Submitting s1 jobs'):\n",
    "        # submit rtc jobs and ask for incidence angle map, in amplitude, @ 30 m resolution\n",
    "        # https://hyp3-docs.asf.alaska.edu/using/sdk_api/#hyp3_sdk.hyp3.HyP3.submit_rtc_job\n",
    "        rtc_jobs += hyp3.submit_rtc_job(g, name = job_name, include_inc_map = True,\\\n",
    "            scale = 'amplitude', dem_matching = False, resolution = 30)\n",
    "\n",
    "    # warn user this may take a few hours for big jobs\n",
    "    print(f'Watching {len(rtc_jobs)} jobs. This may take a while...')\n",
    "\n",
    "    # have hyp3 watch and update progress bar every 60 seconds\n",
    "    hyp3.watch(rtc_jobs)\n",
    "\n",
    "    # refresh jobs list with successes and failures\n",
    "    rtc_jobs = hyp3.refresh(rtc_jobs)\n",
    "\n",
    "    # filter out failed jobs\n",
    "    failed_jobs = rtc_jobs.filter_jobs(succeeded=False, running=False, failed=True)\n",
    "    if len(failed_jobs) > 0:\n",
    "        print(f'{len(failed_jobs)} jobs failed.')\n",
    "    \n",
    "    # return only successful jobs\n",
    "    return rtc_jobs.filter_jobs(succeeded = True)\n",
    "\n",
    "def hyp3_jobs_to_dataArray(jobs: sdk.jobs.Batch, area: shapely.geometry.box, outdir: str, clean = True) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Download rtc Sentinel-1 images from Hyp3 pipeline.\n",
    "    https://hyp3-docs.asf.alaska.edu/using/sdk_api/\n",
    "\n",
    "    Args:\n",
    "    jobs: hyp3 Batch object of completed jobs\n",
    "    outdir: directory to save tif files.\n",
    "    clean: clean up tiffs after creating DataArray [default: True]\n",
    "\n",
    "    Returns:\n",
    "    da: DataArray of Sentinel VV+VH and incidence angle\n",
    "    \"\"\"\n",
    "    # make data directory to store incoming tifs\n",
    "    os.makedirs(outdir, exist_ok = True)\n",
    "    # list to hold new DataArrays from downloaded tiffs\n",
    "    das = []\n",
    "    # list to check if a granule is repeated in the job list\n",
    "    granules = []\n",
    "\n",
    "    for job in tqdm(jobs, desc = 'Downloading S1 images'):\n",
    "        # capture url from job description\n",
    "        u = job.files[0]['url']\n",
    "        # capture granule (for metadata scraping)\n",
    "        granule = job.job_parameters['granules'][0]\n",
    "        # skip this loop if granule is repeated in job list\n",
    "        if granule in granules:\n",
    "            continue\n",
    "        # otherwise append to granules list\n",
    "        granules.append(granule)\n",
    "        # get granule metadata\n",
    "        granule_metadata = asf.product_search(f'{granule}-GRD_HD')[0]\n",
    "        # set flight direction\n",
    "        flight_dir = granule_metadata.properties['flightDirection'].lower()\n",
    "        # set relative orbit \n",
    "        relative_orbit = granule_metadata.properties['pathNumber']\n",
    "        # create dictionary to hold cloud url from .zip url\n",
    "        # this lets us download only VV, VH, inc without getting other data from zip\n",
    "        urls = {}\n",
    "        urls[f'{granule}_VV'] = u.replace('.zip', '_VV.tif')\n",
    "        urls[f'{granule}_VH'] = u.replace('.zip', '_VH.tif')\n",
    "        urls[f'{granule}_inc'] = u.replace('.zip', '_inc_map.tif')\n",
    "        # list to hold each band of image for concating to multi-band image\n",
    "        imgs = []\n",
    "        for name, url in urls.items():\n",
    "            # download url to a tif file\n",
    "            url_download(url, join(outdir, f'{name}.tif'), verbose = False)\n",
    "            # open image in xarray\n",
    "            img = rxa.open_rasterio(join(outdir, f'{name}.tif'))\n",
    "            # reproject to WGS84\n",
    "            img = img.rio.reproject('EPSG:4326')\n",
    "            # clip to user specified area\n",
    "            img = img.rio.clip([area], 'EPSG:4326')\n",
    "            # create band name\n",
    "            band_name = name.replace(f'{granule}_', '')\n",
    "            # add band to image\n",
    "            img = img.assign_coords(band = [band_name])\n",
    "            # add named band image to 3 image stack\n",
    "            imgs.append(img)\n",
    "        # concat VV, VH, and inc into one xarray DataArray\n",
    "        da = xr.concat(imgs, dim = 'band')\n",
    "\n",
    "        # we need to reproject each image to match the first image to make CRSs work\n",
    "        if das:\n",
    "            da = da.rio.reproject_match(das[0])\n",
    "\n",
    "        da = da.expand_dims(dim = {'time': 1})\n",
    "        # add time as a indexable parameter\n",
    "        da = da.assign_coords(time = [pd.to_datetime(granule.split('_')[4])])\n",
    "        # add flight direction as indexable parameter\n",
    "        da = da.assign_coords(flight_dir = ('time', [flight_dir]))\n",
    "        # add platform as indexable parameter\n",
    "        platform = granule[0:3]\n",
    "        da = da.assign_coords(platform = ('time', [platform]))\n",
    "        # add relative orbit as indexable parameter\n",
    "        da = da.assign_coords(relative_orbit = ('time', [relative_orbit]))\n",
    "        # append multi-band image to das list to concat into time-series DataArray\n",
    "        das.append(da)\n",
    "    # take list of multi-band images with different time values and make time series\n",
    "    full_da = xr.concat(das, dim = 'time')\n",
    "\n",
    "    # remove temp directory of tiffs\n",
    "    if clean:\n",
    "        shutil.rmtree(outdir)\n",
    "    # return the full DataArray of time series multi-band (vv, vh, inc) images clipped to region\n",
    "    return full_da\n",
    "\n",
    "def download_s1_imgs(search_results: pd.DataFrame, area: shapely.geometry.box, job_name: str = 'sentinel-1-snow-depth', tmp_dir = './tmp', existing_job_name = False) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Download rtc Sentinel-1 images from Hyp3 pipeline.\n",
    "    https://hyp3-docs.asf.alaska.edu/using/sdk_api/\n",
    "\n",
    "    Args:\n",
    "    search_results: Dataframe of asf_search Sentinel-1 granules to download\n",
    "    job_name: job_name to use for hyp3 cloud processing. [default: 'sentinel-1-snow-depth]\n",
    "    tmp_dir: temporary directory to save tifs to\n",
    "\n",
    "    Returns:\n",
    "    s1_dataset: Xarray dataset of Sentinel-1 backscatter and incidence angle\n",
    "    \"\"\"\n",
    "    # submit asf_search results to the hyp3 pipeline and watch for jobs to run\n",
    "    rtc_jobs = hyp3_pipeline(search_results = search_results, job_name = job_name, existing_job_name = existing_job_name)\n",
    "    # download tiffs from successful hyp3 pipeline and convert to the xarray DataArray\n",
    "    s1_dataArray = hyp3_jobs_to_dataArray(jobs = rtc_jobs, area = area, outdir = tmp_dir, clean = False)\n",
    "    # promote to DataSet and set sentinel 1 image to 's1' data variable\n",
    "    s1_dataset = s1_dataArray.to_dataset(name = 's1', promote_attrs = True)\n",
    "    # s1_units tag\n",
    "    s1_dataset.attrs['s1_units'] = 'amp'\n",
    "    # save to netcdf for testing\n",
    "    # s1_dataset.to_netcdf(out_fp)\n",
    "    return s1_dataset\n",
    "\n",
    "# End of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Dataset\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spicy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13caf6e9649d854b781d0ff5e03ba6873899a9c51dd9632444222443f682bdc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
