{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Main Program: Spicy Snow (MacBook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Add main repo to path\n",
    "import sys\n",
    "from os.path import expanduser\n",
    "sys.path.append(expanduser('../../'))\n",
    "\n",
    "from spicy_snow import retrieve_snow_depth\n",
    "from spicy_snow.IO.user_dates import get_input_dates\n",
    "\n",
    "import shapely\n",
    "\n",
    "# Provide bounding box (EPSG:4326 user-provided coordinates)\n",
    "area = shapely.geometry.box(-106.9, 36.9, -106.4, 37.4)\n",
    "\n",
    "# Get tuple of dates. Provided date is ending date and start date is always prior August 1st\n",
    "dates = get_input_dates(\"2024-06-01\", \"2023-11-01\")\n",
    "\n",
    "# Function to actually get data, run processing, returns xarray dataset w/ daily time dimension\n",
    "#s1_sd = get_s1_snow_depth(area, dates, work_dir = './contrib/data/BPR_retrieval/')\n",
    "# s1_sd = retrieve_snow_depth(area, dates, work_dir = 'Users/Adrian/Desktop/BPR_retrieval/')\n",
    "s1_sd = retrieve_snow_depth(area, dates, work_dir = '/Users/marzi/Desktop/SpicySnow/')\n",
    "# s1_sd = retrieve_snow_depth(area, dates, work_dir = './BPR_retrieval/')\n",
    "\n",
    "# work_dir will be created if not present \n",
    "# optional keyword ideas: job_name, fitting parameters (A, B, C), exisiting_job_name, outfp\n",
    "# `outfp = './idaho_ret.nc` will output datset to netcdf\n",
    "\n",
    "# plot first day of 2020 to check data quality\n",
    "# s1_sd.sel(time = \"2022-01-01\").plot()\n",
    "\n",
    "# save as pickle file\n",
    "# dump completed dataset to data directory\n",
    "# with open('./BPR_retrieval/spicy_test.pkl', 'wb') as f:\n",
    "    # pickle.dump(ds, f)\n",
    "\n",
    "#s1_sd.to_netcdf(f'./BPR_snd2022.nc')\n",
    "s1_sd.to_netcdf(f'/Users/marzi/Desktop/BPR_snd2024.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear temp files ... (MacBook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear temporary files\n",
    "\n",
    "import os, shutil\n",
    "\n",
    "# Imput directory\n",
    "folder = '/Users/marzi/Desktop/SpicySnow/tmp'\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "print('tmp folder cleared!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Main Program: Spicy Snow (PC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Add main repo to path\n",
    "import sys\n",
    "from os.path import expanduser\n",
    "sys.path.append(expanduser('../../'))\n",
    "\n",
    "from spicy_snow import retrieve_snow_depth\n",
    "from spicy_snow.IO.user_dates import get_input_dates\n",
    "\n",
    "import shapely\n",
    "\n",
    "# Provide bounding box (EPSG:4326 user-provided coordinates)\n",
    "area = shapely.geometry.box(-106.9, 36.9, -106.4, 37.4)\n",
    "\n",
    "# Get tuple of dates. Provided date is ending date and start date is always prior August 1st\n",
    "dates = get_input_dates(\"2024-06-01\", \"2023-11-01\")\n",
    "\n",
    "# Function to actually get data, run processing, returns xarray dataset w/ daily time dimension\n",
    "#s1_sd = get_s1_snow_depth(area, dates, work_dir = './contrib/data/BPR_retrieval/')\n",
    "# s1_sd = retrieve_snow_depth(area, dates, work_dir = 'Users/Adrian/Desktop/BPR_retrieval/')\n",
    "s1_sd = retrieve_snow_depth(area, dates, work_dir = 'C:/Users/marzi/Desktop/SpicySnow/')\n",
    "# s1_sd = retrieve_snow_depth(area, dates, work_dir = './BPR_retrieval/')\n",
    "\n",
    "# work_dir will be created if not present \n",
    "# optional keyword ideas: job_name, fitting parameters (A, B, C), exisiting_job_name, outfp\n",
    "# `outfp = './idaho_ret.nc` will output datset to netcdf\n",
    "\n",
    "# plot first day of 2020 to check data quality\n",
    "# s1_sd.sel(time = \"2022-01-01\").plot()\n",
    "\n",
    "# save as pickle file\n",
    "# dump completed dataset to data directory\n",
    "# with open('./BPR_retrieval/spicy_test.pkl', 'wb') as f:\n",
    "    # pickle.dump(ds, f)\n",
    "\n",
    "#s1_sd.to_netcdf(f'./BPR_snd2022.nc')\n",
    "s1_sd.to_netcdf(f'C:/Users/marzi/Desktop/BPR_snd2024.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clear temp files ... (PC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear temporary files\n",
    "\n",
    "import os, shutil\n",
    "\n",
    "# Imput directory\n",
    "folder = 'C:/Users/marzi/Desktop/SpicySnow/tmp'\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "print('tmp folder cleared!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES\n",
    "# From README file ...\n",
    "\n",
    "# Function to actually get data, run processing, returns xarray dataset w/ daily time dimension\n",
    "s1_sd = get_s1_snow_depth(area, dates, work_dir = './idaho_retrieval/)\n",
    "# Should 'get_s1_snow_depth' be 'retrieve_snow_depth'?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowenvMAC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
