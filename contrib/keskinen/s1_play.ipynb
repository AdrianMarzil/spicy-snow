{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asf_search as asf\n",
    "import pandas as pd\n",
    "import shapely.geometry\n",
    "import xarray\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from spicy_snow.download.sentinel1 import s1_img_search#, download_s1_imgs\n",
    "\n",
    "import os\n",
    "os.environ['PROJ_LIB'] = '/Users/zachkeskinen/miniconda3/pkgs/proj-9.1.0-hf909084_1/share/proj'\n",
    "\n",
    "import pyproj\n",
    "pyproj.datadir.set_data_dir('/Users/zachkeskinen/miniconda3/pkgs/proj-9.1.0-hf909084_1/share/proj')\n",
    "pyproj._pyproj_global_context_initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import box\n",
    "area = box(-114.4, 43, -114.3, 43.1)\n",
    "dates = ('2019-12-28', '2020-02-02')\n",
    "# search_result = s1_img_search(area, dates)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Download from cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyp3_sdk.exceptions import AuthenticationError\n",
    "import hyp3_sdk as sdk\n",
    "try:\n",
    "    # .netrc\n",
    "    hyp3 = sdk.HyP3()\n",
    "except AuthenticationError:\n",
    "    # prompt for password\n",
    "    hyp3 = sdk.HyP3(prompt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = hyp3.find_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp3.watch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granules = search_result['properties.sceneName'].iloc[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "list2d = [i.job_parameters['granules'] for i in batch.filter_jobs(succeeded = True, failed = False, running = False)]\n",
    "merged = list(itertools.chain(*list2d))\n",
    "len(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtc_jobs = hyp3.find_jobs(name = 'refresh_test_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granules = search_result['properties.sceneName'].iloc[2:4]\n",
    "\n",
    "rtc_jobs = sdk.Batch()\n",
    "for g in granules:\n",
    "    # https://hyp3-docs.asf.alaska.edu/using/sdk_api/#hyp3_sdk.hyp3.HyP3.submit_rtc_job\n",
    "    rtc_jobs += hyp3.submit_rtc_job(g, name = 'cloud-download-test-v2', include_inc_map = True,\\\n",
    "        scale = 'amplitude', dem_matching = False, resolution = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp3._watch_batch(rtc_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "pbar = tqdm(total = len(rtc_jobs))\n",
    "while not rtc_jobs.complete():\n",
    "    # to get updated information\n",
    "    pbar.update(len(rtc_jobs.filter_jobs(succeeded=True, running=False, failed=True)))\n",
    "    rtc_jobs = hyp3.refresh(rtc_jobs)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "pbar = tqdm(total = len(rtc_jobs))\n",
    "while not rtc_jobs.complete():\n",
    "    # to get updated information\n",
    "    pbar.update(len(rtc_jobs.filter_jobs(succeeded=True, running=False, failed=True)))\n",
    "    rtc_jobs = hyp3.refresh(rtc_jobs)\n",
    "pbar.close()\n",
    "    \n",
    "succeeded_jobs = rtc_jobs.filter_jobs(succeeded=True, running=False, failed=False)\n",
    "print(f'Number of succeeded jobs: {len(succeeded_jobs)}')\n",
    "failed_jobs = rtc_jobs.filter_jobs(succeeded=False, running=False, failed=True)\n",
    "print(f'Number of failed jobs: {len(failed_jobs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = rtc_jobs[0].files[0]['url']\n",
    "vv_url = u.replace('.zip', '_VV.tif')\n",
    "vh_url = u.replace('.zip', '_VH.tif')\n",
    "inc_url = u.replace('.zip', '_inc_map.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def reporthook(count, block_size, total_size):\n",
    "    global start_time\n",
    "    if count == 0:\n",
    "        start_time = time.time()\n",
    "        return\n",
    "    duration = time.time() - start_time\n",
    "    progress_size = int(count * block_size)\n",
    "    speed = min(int(count*block_size*100/total_size),100)\n",
    "    percent = int(count * block_size * 100 / total_size)\n",
    "    sys.stdout.write(\"\\r...%d%%, %d MB, %d KB/s, %d seconds passed\" %\n",
    "                    (percent, progress_size / (1024 * 1024), speed, duration))\n",
    "    sys.stdout.flush()\n",
    "from os.path import basename, exists\n",
    "def url_download(url, out_fp, overwrite = False):\n",
    "    if not exists(out_fp) or overwrite == True:\n",
    "        print(f'Downloading {basename(out_fp)}.')\n",
    "        urlretrieve(url, out_fp, reporthook)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, job in enumerate(rtc_jobs[:]):\n",
    "    u = job.files[0]['url']\n",
    "    granule = job.job_parameters['granules'][0]\n",
    "    print(job.job_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import rioxarray as rxa\n",
    "import pandas as pd\n",
    "das = []\n",
    "granules = []\n",
    "for i, job in enumerate(rtc_jobs[:3]):\n",
    "    u = job.files[0]['url']\n",
    "    granule = job.job_parameters['granules'][0]\n",
    "    if granule in granules:\n",
    "        continue\n",
    "    granules.append(granule)\n",
    "    urls = {}\n",
    "    urls[f'{granule}_VV'] = u.replace('.zip', '_VV.tif')\n",
    "    urls[f'{granule}_VH'] = u.replace('.zip', '_VH.tif')\n",
    "    urls[f'{granule}_inc'] = u.replace('.zip', '_inc_map.tif')\n",
    "\n",
    "    imgs = []\n",
    "    for j, (name, url) in enumerate(urls.items()):\n",
    "        url_download(url, f'data/{name}.tif')\n",
    "        img = rxa.open_rasterio(f'data/{name}.tif')\n",
    "        img = img.rio.clip([area], 'EPSG:4326')\n",
    "        img = img.rio.reproject('EPSG:4326')\n",
    "        band_name = name.replace(f'{granule}_', '')\n",
    "        img = img.assign_coords(time = pd.to_datetime(granule.split('_')[4]))\n",
    "        imgs.append(img.assign_coords(band = [band_name]))\n",
    "    da = xr.concat(imgs, dim = 'band')\n",
    "    if i != 0:\n",
    "        da = da.rio.reproject_match(das[0])\n",
    "    das.append(da)\n",
    "ds = xr.concat(das, dim = 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.concat([das[0], das[1].rio.reproject_match(das[0]),das[2].rio.reproject_match(das[0])], dim = 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing results between sentinel sat and asf_search. Both seem the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = shapely.geometry.box(43, 6, 43.1, 6.1)\n",
    "dates = ('2018-12-28T00:00:00.000Z', '2020-01-02T00:00:00.000Z')\n",
    "search_results = asf.geo_search(platform = [asf.PLATFORM.SENTINEL1], intersectsWith = area.wkt,\\\n",
    "        start = dates[0], end = dates[1], processingLevel = asf.PRODUCT_TYPE.GRD_HD)\n",
    "print(len(search_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentinelsat import SentinelAPI, read_geojson, geojson_to_wkt\n",
    "dates = ('2018-12-28T00:00:00.000Z', '2020-01-02T00:00:00.000Z')\n",
    "api = SentinelAPI('zachkeskinen', 'DR1seuss')\n",
    "footprint = area.wkt\n",
    "products = api.query(footprint, platformname = 'Sentinel-1', producttype = 'GRD', beginposition = dates,\n",
    "                     polarisationmode = 'VV+VH', limit=1000)\n",
    "print(len(products))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area.bounds[3] < 90\n",
    "area.bounds[1] > 0\n",
    "area.bounds[2] < 180\n",
    "area.bounds[0] > -180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asf_search as asf\n",
    "import pandas as pd\n",
    "import shapely.geometry\n",
    "import xarray\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from spicy_snow.download.sentinel1 import s1_img_search#, download_s1_imgs\n",
    "\n",
    "import os\n",
    "os.environ['PROJ_LIB'] = '/Users/zachkeskinen/miniconda3/pkgs/proj-9.1.0-hf909084_1/share/proj'\n",
    "\n",
    "import pyproj\n",
    "pyproj.datadir.set_data_dir('/Users/zachkeskinen/miniconda3/pkgs/proj-9.1.0-hf909084_1/share/proj')\n",
    "pyproj._pyproj_global_context_initialize()\n",
    "\n",
    "import rioxarray as rxa\n",
    "img = rxa.open_rasterio('/Users/zachkeskinen/Documents/spicy-snow/data/fcf.tif', chunk = True)\n",
    "from shapely.geometry import box\n",
    "area = box(-114.4, 43, -114.3, 43.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import rioxarray as rxa\n",
    "import sys\n",
    "from os.path import expanduser\n",
    "sys.path.append(expanduser('~/Documents/spicy-snow'))\n",
    "\n",
    "from spicy_snow.download.sentinel1 import s1_img_search, download_s1_imgs\n",
    "from spicy_snow.download.forest_cover import download_fcf, add_fcf\n",
    "from spicy_snow.download.snow_cover import download_snow_cover\n",
    "with open('/Users/zachkeskinen/Documents/spicy-snow/data/all_ds.pkl', 'rb') as f:\n",
    "    ds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = download_snow_cover(ds, tmp_dir = '/Users/zachkeskinen/Documents/spicy-snow/data/tmp/', clean = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.isel(time = 1)['ims']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/zachkeskinen/Documents/spicy-snow/data/imsds.pkl', 'rb') as f:\n",
    "    ims = pickle.dump(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims = rxa.open_rasterio('/Users/zachkeskinen/Documents/spicy-snow/data/tmp/ims2019281_1km_v1.3.nc', decode_times=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims = ims.rio.reproject('EPSG:4326').rio.clip_box(-114.4, 43, -114.3, 43.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.isel(time = 0)['ims'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/zachkeskinen/Documents/spicy-snow/data/s1_dt.pkl', 'rb') as f:\n",
    "    ds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spicy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "d55f2b22363d79254ff041d13471de54e352f3ae9dfa1886ee2b85fe903b5a57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
