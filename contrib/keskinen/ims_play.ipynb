{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://usicecenter.gov/Products/ImsHome\n",
    "\n",
    "https://nsidc.org/data/user-resources/help-center/how-access-data-using-ftp-client-command-line-wget-or-python\n",
    "\n",
    "https://www.itsonlyamodel.us/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ftplib import FTP\n",
    "import os\n",
    "from os.path import join\n",
    "from datetime import datetime\n",
    "import rioxarray as rio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rioxarray as rxa\n",
    "import urllib.request\n",
    "import gzip\n",
    "import datetime\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray as rxa\n",
    "from pyproj import Transformer\n",
    "\n",
    "def decompress(infile, tofile):\n",
    "    with open(infile, 'rb') as inf, open(tofile, 'wb') as ouf:\n",
    "        decom_str = gzip.decompress(inf.read())\n",
    "        ouf.write(decom_str)\n",
    "\n",
    "        return tofile\n",
    "\n",
    "def get_ims_day_data(year: str, doy: str, tmp_dir: str, clean: bool = True) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Download and decompress one days worth of IMS data.\n",
    "\n",
    "    Args:\n",
    "    year: Year of the data you want.\n",
    "    doy: Calendar day of year you want in 'DDD' format. Range is 001 - 366.\n",
    "    \"\"\"\n",
    "    os.makedirs(tmp_dir, exist_ok = True)\n",
    "    os.chdir(tmp_dir)\n",
    "    local_fp, _ = urllib.request.urlretrieve(f'ftp://sidads.colorado.edu/pub/DATASETS/NOAA/G02156/netcdf/1km/{year}/ims{year}{doy}_1km_v1.3.nc.gz', f'ims{year}{doy}_1km_v1.3.nc.gz')\n",
    "    out_file = decompress(local_fp, local_fp.replace('.gz',''))\n",
    "    ims = rxa.open_rasterio(out_file, decode_times = False)\n",
    "\n",
    "    if clean:\n",
    "        shutil.rmtree(tmp_dir)\n",
    "    return ims\n",
    "\n",
    "def download_snow_cover(dataset: xr.Dataset, dates: (str, str), tmp_dir = './tmp') -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Download IMS snow-cover images.\n",
    "\n",
    "    Args:\n",
    "    dataset: Full dataset to add IMS data to\n",
    "    dates: Tuple of dates to get data between (YYYY-MM-DD)\n",
    "    tmp_dir: filepath to save temporary downloads to [default: './tmp']\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    dates = [{'date': datetime.strptime(d, '%Y-%m-%d'), 'year': int(d.split('-')[0]), 'doy':datetime.strptime(d, '%Y-%m-%d').timetuple().tm_yday} for d in dates]\n",
    "    assert dates[0]['date'] < dates[1]['date']\n",
    "    for year in range(dates[0]['year'], dates[1]['year']+1):\n",
    "        print(year)\n",
    "        days = list(range(1, 366))\n",
    "        if year == dates[0]['year']:\n",
    "            days = [d for d in days if d > dates[0]['doy']]\n",
    "        elif year == dates[1]['year']:\n",
    "            days = [d for d in days if d < dates[1]['doy']+1]\n",
    "        for day in days:\n",
    "            print(day)\n",
    "            try:\n",
    "                date = pd.to_datetime(f'{year} {day}', format = '%Y %j')\n",
    "                ims = get_ims_day_data(year, f'{day:03}', tmp_dir = tmp_dir, clean = False) #revert to clean = True at somepoint\n",
    "                dataset = add_ims_data(dataset, ims, date)\n",
    "            except URLError as e:\n",
    "                print(e)\n",
    "                print(f'Missing {date.date()}')\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def add_ims_data(dataset: xr.Dataset, ims: xr.DataArray, date: pd.Timestamp) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Add xarray dataArray of IMS data to a larger xarray Dataset.\n",
    "\n",
    "    Args:\n",
    "    dataset: Xarray dataset to add IMS data to\n",
    "    ims: IMS dataArray for one days worth of data\n",
    "    date: Date of IMS retrieval\n",
    "    \"\"\"\n",
    "    transformer = Transformer.from_crs(4326, 9001, always_xy=True)\n",
    "    polar_bounds = transformer.transform(*dataset['s1'].rio.bounds())\n",
    "    ims = ims.rio.clip_box(*polar_bounds)\n",
    "    ims = ims.rio.reproject_match(dataset['s1'])\n",
    "    ims = ims.assign_coords(time = [date])\n",
    "    dataset = xr.merge([dataset, ims.rename('ims')])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/Users/zachkeskinen/Documents/spicy-snow/tests/test_data/s1_da.pkl', 'rb') as f:\n",
    "    da = pickle.load(f)\n",
    "ds = da.to_dataset(name = 's1', promote_attrs = True)\n",
    "days = [pd.to_datetime(d) for d in ds.time.values]\n",
    "for day in days:\n",
    "    print(day)\n",
    "    tmp_dir= '/Users/zachkeskinen/Documents/spicy-snow/data/tmp'\n",
    "    ims = get_ims_day_data(day.year, f'{day.day:03}', tmp_dir = tmp_dir, clean = False) #revert to clean = True at somepoint\n",
    "    ds = add_ims_data(ds, ims, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import netCDF4\n",
    "\n",
    "def open_netcdf(fname):\n",
    "    if fname.endswith(\".gz\"):\n",
    "        infile = gzip.open(fname, 'rb')\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False)\n",
    "        shutil.copyfileobj(infile, tmp)\n",
    "        infile.close()\n",
    "        tmp.close()\n",
    "        data = netCDF4.Dataset(tmp.name)\n",
    "        os.unlink(tmp.name)\n",
    "    else:\n",
    "        data = netCDF4.Dataset(fname)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "def parse_lines(file_path):\n",
    "    with open(file_path) as f:  \n",
    "        # read the content of the file opened\n",
    "        lines = f.readlines()\n",
    "    lines = [l.strip().strip().split(':', 1) for l in lines]\n",
    "    lines = {l[0].strip():l[1].strip() for l in lines if len(l) == 2}\n",
    "    return lines\n",
    "\n",
    "def read_data_1km_compressed(filename):\n",
    "  nx = 8000\n",
    "  widths = np.full((nx), 1, dtype=int).tolist()\n",
    "  data = pd.read_fwf(filename, widths=widths, lineterminator='\\n', header=None).values\n",
    "  return(data)\n",
    "\n",
    "def read_data_1km(year=2017, doy=300):\n",
    "  nx = 24576\n",
    "  url = (\"ftp://sidads.colorado.edu/pub/DATASETS/NOAA/G02156/netcdf/1km/%s/ims%s%s_1km_v1.3.nc.gz\" %\n",
    "         (year, year, doy))\n",
    "  widths = np.full((nx), 1, dtype=int).tolist()\n",
    "  print(url)\n",
    "  # data = pd.read_fwf(url, skiprows=30, widths=widths,\n",
    "  #                             lineterminator='\\n', header=None, compression='gzip').values\n",
    "  with gzip.open(url, 'rb') as f_in:\n",
    "    with open('/Users/zachkeskinen/Documents/spicy-snow/data/ims_tmp.nc', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2017\n",
    "doy = '001'\n",
    "local_fp, _ = urllib.request.urlretrieve(f'ftp://sidads.colorado.edu/pub/DATASETS/NOAA/G02156/netcdf/1km/{year}/ims{year}{doy}_1km_v1.3.nc.gz', f'ims{year}{doy}_1km_v1.3.nc.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rxa.open_rasterio('/Users/zachkeskinen/Documents/spicy-snow/data/tmp/ims2019363_1km_v1.3.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import gzip\n",
    "import datetime\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray as rxa\n",
    "from pyproj import Transformer\n",
    "\n",
    "def decompress(infile, tofile):\n",
    "    with open(infile, 'rb') as inf, open(tofile, 'wb') as ouf:\n",
    "        decom_str = gzip.decompress(inf.read())\n",
    "        ouf.write(decom_str)\n",
    "\n",
    "        return tofile\n",
    "\n",
    "def get_ims_data(year, doy):\n",
    "    local_fp, _ = urllib.request.urlretrieve(f'ftp://sidads.colorado.edu/pub/DATASETS/NOAA/G02156/netcdf/1km/{year}/ims{year}{doy}_1km_v1.3.nc.gz', f'ims{year}{doy}_1km_v1.3.nc.gz')\n",
    "    out_file = decompress(local_fp, local_fp.replace('.gz',''))\n",
    "    return out_file\n",
    "\n",
    "def add_ims_data(dataset, ims, date):\n",
    "    transformer = Transformer.from_crs(4326, 9001, always_xy=True)\n",
    "    polar_bounds = transformer.transform(*dataset.rio.bounds())\n",
    "    ims = ims.rio.clip_box(*polar_bounds)\n",
    "    ims = ims.rio.reproject_match(dataset)\n",
    "    ims = ims.assign_coords(time = [date])\n",
    "    dataset = xr.merge([dataset, ims.rename('ims')])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# import pickle\n",
    "# with open('/Users/zachkeskinen/Documents/spicy-snow/tests/test_data/s1_da.pkl', 'rb') as f:\n",
    "#     da = pickle.load(f)\n",
    "# ds = da.to_dataset(name = 's1', promote_attrs = True)\n",
    "# out_ims = get_ims_data(2017, '033')\n",
    "# ims = rxa.open_rasterio(out_ims)\n",
    "# doy = int('032')\n",
    "# date = pd.to_datetime(f'{2017} {doy}', format = '%Y %j')\n",
    "# ds = add_ims_data(ds, ims, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ims_day_data(year: str, doy: str, tmp_dir: str, clean: bool = True) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Download and decompress one days worth of IMS data.\n",
    "\n",
    "    Args:\n",
    "    year: Year of the data you want.\n",
    "    doy: Calendar day of year you want in 'DDD' format. Range is 001 - 366.\n",
    "    \"\"\"\n",
    "    os.makedirs(tmp_dir, exist_ok = True)\n",
    "    os.chdir(tmp_dir)\n",
    "    local_fp, _ = urllib.request.urlretrieve(f'ftp://sidads.colorado.edu/pub/DATASETS/NOAA/G02156/netcdf/1km/{year}/ims{year}{doy}_1km_v1.3.nc.gz', f'ims{year}{doy}_1km_v1.3.nc.gz')\n",
    "    out_file = decompress(local_fp, local_fp.replace('.gz',''))\n",
    "    ims = rxa.open_rasterio(out_file, decode_times = False)\n",
    "\n",
    "    if clean:\n",
    "        shutil.rmtree(tmp_dir)\n",
    "    return ims\n",
    "\n",
    "def download_snow_cover(dataset: xr.Dataset, dates: (str, str), tmp_dir = './tmp') -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Download IMS snow-cover images.\n",
    "\n",
    "    Args:\n",
    "    dataset: Full dataset to add IMS data to\n",
    "    dates: Tuple of dates to get data between (YYYY-MM-DD)\n",
    "    tmp_dir: filepath to save temporary downloads to [default: './tmp']\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    dates = [{'date': datetime.strptime(d, '%Y-%m-%d'), 'year': int(d.split('-')[0]), 'doy':datetime.strptime(d, '%Y-%m-%d').timetuple().tm_yday} for d in dates]\n",
    "    assert dates[0]['date'] < dates[1]['date']\n",
    "    for year in range(dates[0]['year'], dates[1]['year']+1):\n",
    "        print(year)\n",
    "        days = list(range(1, 366))\n",
    "        if year == dates[0]['year']:\n",
    "            days = [d for d in days if d > dates[0]['doy']]\n",
    "        elif year == dates[1]['year']:\n",
    "            days = [d for d in days if d < dates[1]['doy']+1]\n",
    "        for day in days:\n",
    "            print(day)\n",
    "            try:\n",
    "                date = pd.to_datetime(f'{year} {day}', format = '%Y %j')\n",
    "                ims = get_ims_day_data(year, f'{day:03}', tmp_dir = tmp_dir, clean = False) #revert to clean = True at somepoint\n",
    "                dataset = add_ims_data(dataset, ims, date)\n",
    "            except URLError as e:\n",
    "                print(e)\n",
    "                print(f'Missing {date.date()}')\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def add_ims_data(dataset: xr.Dataset, ims: xr.DataArray, date: pd.Timestamp) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Add xarray dataArray of IMS data to a larger xarray Dataset.\n",
    "\n",
    "    Args:\n",
    "    dataset: Xarray dataset to add IMS data to\n",
    "    ims: IMS dataArray for one days worth of data\n",
    "    date: Date of IMS retrieval\n",
    "    \"\"\"\n",
    "    print('Transforming coords')\n",
    "    transformer = Transformer.from_crs(4326, 9001, always_xy=True)\n",
    "    polar_bounds = transformer.transform(*dataset['s1'].rio.bounds())\n",
    "    print('Clipping')\n",
    "    ims = ims.rio.clip_box(*polar_bounds)\n",
    "    print('Reprojecting')\n",
    "    ims = ims.rio.reproject_match(dataset['s1'])\n",
    "    ims = ims.assign_coords(time = [date])\n",
    "    print('adding')\n",
    "    dataset = xr.merge([dataset, ims.rename('ims')])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/Users/zachkeskinen/Documents/spicy-snow/tests/test_data/s1_da.pkl', 'rb') as f:\n",
    "    da = pickle.load(f)\n",
    "ds = da.to_dataset(name = 's1', promote_attrs = True)\n",
    "days = [pd.to_datetime(ds.time.values[0]) for d in ds.time.values]\n",
    "for day in days:\n",
    "    print(day)\n",
    "    tmp_dir= '/Users/zachkeskinen/Documents/spicy-snow/data/tmp'\n",
    "    ims = get_ims_day_data(day.year, f'{day.day:03}', tmp_dir = tmp_dir, clean = False) #revert to clean = True at somepoint\n",
    "    ds = add_ims_data(ds, ims, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/zachkeskinen/Documents/spicy-snow/data/ims_v1.pkl', 'rb') as f:\n",
    "    ds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['ims']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer.from_crs(4326, 9001, always_xy=True)\n",
    "polar_bounds = transformer.transform(*ds.rio.bounds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "\n",
    "mysock = urllib.urlopen('ftp://sidads.colorado.edu/pub/DATASETS/NOAA/G02156/netcdf/1km/2017/ims2017001_1km_v1.3.nc.gz')\n",
    "memfile = io.BytesIO(mysock.read())\n",
    "with ZipFile(memfile, 'r') as myzip:\n",
    "    f = myzip.open('eggs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fpt_download(domain = \"ftp://sidads.colorado.edu/pub/DATASETS/NOAA/G02156/netcdf/1km/\"):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data_1km()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The following 3 variables can be changed ###\n",
    "# 1. Set the directory you would like to download the files to\n",
    "destdir='/Users/zachkeskinen/Documents/c-snow/contrib/keskinen/data'\n",
    "\n",
    "# 2. Set the path to the FTP directory that contains the data you wish to download.\n",
    "# This example is for the daily northern hemisphere data from the Sea Ice Index\n",
    "# https://nsidc.org/data/g02135\n",
    "directory = '/DATASETS/NOAA/G02156/1km/'\n",
    "\n",
    "# 3. Set the password which will be your email address\n",
    "password = 'zachkeskinen@gmail.com'\n",
    "\n",
    "# 4. Set dates to get IMS from\n",
    "dates = ('2019-12-20', '2020-01-10')\n",
    "dates = [{'date': datetime.strptime(d, '%Y-%m-%d'), 'year': int(d.split('-')[0]), 'doy':datetime.strptime(d, '%Y-%m-%d').timetuple().tm_yday} for d in dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "dates = ('2019-12-20', '2020-01-10')\n",
    "dates = [{'date': datetime.strptime(d, '%Y-%m-%d'), 'year': int(d.split('-')[0]), 'doy':datetime.strptime(d, '%Y-%m-%d').timetuple().tm_yday} for d in dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Don't need to change this code below ###\n",
    "############################################\n",
    "# FTP server\n",
    "ftpdir = 'sidads.colorado.edu'\n",
    "\n",
    "#Connect and log in to the FTP\n",
    "print('Logging in')\n",
    "ftp = FTP(ftpdir)\n",
    "ftp.login('anonymous',password)\n",
    "\n",
    "#Change to the destination directory on own computer where you want to save the files\n",
    "os.chdir(destdir)\n",
    "\n",
    "assert dates[0]['date'] < dates[1]['date']\n",
    "\n",
    "all_files = []\n",
    "# Change to the directory where the files are on the FTP\n",
    "for year in range(dates[0]['year'], dates[1]['year']+1):\n",
    "    print('Changing to '+ join(directory, str(year)))\n",
    "    ftp.cwd(join(directory, str(year)))\n",
    "\n",
    "    # Get a list of the files in the FTP directory\n",
    "    files = ftp.nlst()\n",
    "    files = files[2:]\n",
    "    if year == dates[0]['year']:\n",
    "        files = [f for f in files if int(f.split('_')[0][-3:]) > dates[0]['doy']]\n",
    "    elif year == dates[1]['year']:\n",
    "        files = [f for f in files if int(f.split('_')[0][-3:]) < dates[1]['doy']]\n",
    "\n",
    "    # #Download all the files within the FTP directory\n",
    "    for f in files:\n",
    "        print('Downloading...' + f)\n",
    "        ftp.retrbinary('RETR ' + f, open(f, 'wb').write)\n",
    "\n",
    "#Close the FTP connection\n",
    "ftp.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'{1:03}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "desc = parse_lines('/Users/zachkeskinen/Documents/c-snow/contrib/keskinen/data/ims2019355_00UTC_1km_v1.3.asc')\n",
    "rows, cols = desc['Dimensions'].split()\n",
    "rows, cols = int(rows), int(cols)\n",
    "ascii_grid = np.loadtxt(\"/Users/zachkeskinen/Documents/c-snow/contrib/keskinen/data/ims2019355_00UTC_1km_v1.3.asc\", skiprows=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.genfromtxt(\"/Users/zachkeskinen/Documents/c-snow/contrib/keskinen/data/ims2019355_00UTC_1km_v1.3.asc\", skip_header=30, delimiter = 1, dtype = int).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spicy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "d55f2b22363d79254ff041d13471de54e352f3ae9dfa1886ee2b85fe903b5a57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
