{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from datetime import datetime\n",
    "sys.path.append('../../')\n",
    "\n",
    "from spicy_snow.processing.snow_index import calc_delta_cross_ratio, calc_delta_gamma, \\\n",
    "    clip_delta_gamma_outlier, calc_snow_index, calc_snow_index_to_snow_depth\n",
    "from spicy_snow.processing.wet_snow import id_newly_wet_snow, id_wet_negative_si, \\\n",
    "    id_newly_frozen_snow, flag_wet_snow\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seperate out training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/56257429/randomly-mask-set-nan-x-of-data-points-in-huge-xarray-dataarray\n",
    "\n",
    "train_dir = Path('../../data/bootstrap/training')\n",
    "val_dir = Path('../../data/bootstrap/validation')\n",
    "\n",
    "train_dir.mkdir(parents = True, exist_ok = True)\n",
    "val_dir.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "for fp in Path('../../Lidar_s1_stacks').glob('*.nc'):\n",
    "    ds = xr.open_dataset(fp)\n",
    "    mask = xr.zeros_like(ds)['fcf'].rename('mask')\n",
    "    mask.data = np.random.rand(*mask.data.shape) < 0.2\n",
    "    mask = mask.broadcast_like(ds['s1'])\n",
    "    train = ds.where(~mask)\n",
    "    val = ds.where(mask)\n",
    "\n",
    "    train.to_netcdf(train_dir.joinpath(fp.name.replace('.nc','.train.nc')))\n",
    "    val.to_netcdf(val_dir.joinpath(fp.name.replace('.nc','.val.nc')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loop through .nc and resample and calculate all possible retrieved snowdepths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "site_dir = Path('/Users/zachkeskinen/Documents/spicy-snow/scripts/optimize/param_sds/Mores_2021-03-15')\n",
    "ds = xr.open_dataset(next(site_dir.glob('*')))\n",
    "idx = ds['lidar-sd'].rename('mask')\n",
    "idx.data = np.random.rand(*idx.data.shape) < 0.2\n",
    "idx = idx.broadcast_like(ds)\n",
    "for fp in site_dir.glob('*'):\n",
    "    ds = xr.open_dataset(fp)\n",
    "    a,b,c = fp.stem.split('_')\n",
    "    train = ds.where(~idx)\n",
    "    val = ds.where(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely import wkt\n",
    "from shapely.geometry import box\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from spicy_snow.processing.snow_index import calc_delta_cross_ratio, calc_delta_gamma, \\\n",
    "    clip_delta_gamma_outlier, calc_snow_index, calc_snow_index_to_snow_depth\n",
    "from spicy_snow.processing.wet_snow import id_newly_wet_snow, id_wet_negative_si, \\\n",
    "    id_newly_frozen_snow, flag_wet_snow\n",
    "\n",
    "# Create parameter space\n",
    "A = np.arange(1, 3.1, 0.5)\n",
    "B = np.arange(0, 1.01, 0.1)\n",
    "C = np.arange(0, 1.001, 0.01)\n",
    "\n",
    "files = Path('../../data/bootstrap/training/').glob('*.nc')\n",
    "for f in files:\n",
    "\n",
    "    # get dataset\n",
    "    ds_name = f.name.split('stacks/')[-1].split('.')[0]\n",
    "    print(datetime.now(), f' -- starting {ds_name}')\n",
    "    ds_ = xr.open_dataset(f).load()\n",
    "    dataset = ds_[['s1','deltaVV','ims','fcf','lidar-sd']]\n",
    "\n",
    "    # find closest timestep to lidar\n",
    "    td = abs(pd.to_datetime(dataset.time) - pd.to_datetime(dataset.attrs['lidar-flight-time']))\n",
    "    closest_ts = dataset.time[np.argmin(td)]\n",
    "\n",
    "    # Brute-force processing loop\n",
    "    for a in tqdm(A):\n",
    "        # print(f'A: {a}')\n",
    "        a = np.round(a, 2)\n",
    "        ds = calc_delta_cross_ratio(dataset, A = a)\n",
    "        for b in B:\n",
    "            b = np.round(b, 2)\n",
    "            # print(f'    B: {b}')\n",
    "            ds = calc_delta_gamma(ds, B = b, inplace=False)\n",
    "            ds = clip_delta_gamma_outlier(ds)\n",
    "            ds = calc_snow_index(ds)\n",
    "            ds = id_newly_wet_snow(ds)\n",
    "            ds = id_wet_negative_si(ds)\n",
    "            ds = id_newly_frozen_snow(ds)\n",
    "            ds = flag_wet_snow(ds)\n",
    "            for c in C:\n",
    "                c = np.round(c, 2)\n",
    "                # print(f'        c: {c}')\n",
    "                # print(f'A={a}; B={b}; C={c}')\n",
    "\n",
    "                ds = calc_snow_index_to_snow_depth(ds, C = c)\n",
    "\n",
    "                sub = ds.sel(time = closest_ts)[['snow_depth', 'wet_snow', 'lidar-sd']]\n",
    "                sub.to_netcdf(f'./param_sds/{ds_name}/{a}_{b}_{c}.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spicy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
