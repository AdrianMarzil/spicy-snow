{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from datetime import datetime\n",
    "sys.path.append('../../')\n",
    "\n",
    "from spicy_snow.processing.snow_index import calc_delta_cross_ratio, calc_delta_gamma, \\\n",
    "    clip_delta_gamma_outlier, calc_snow_index, calc_snow_index_to_snow_depth\n",
    "from spicy_snow.processing.wet_snow import id_newly_wet_snow, id_wet_negative_si, \\\n",
    "    id_newly_frozen_snow, flag_wet_snow\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1681795189"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "np.random.seed(int(time.time()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seperate out training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m train \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mwhere(\u001b[39m~\u001b[39mmask)\n\u001b[1;32m     15\u001b[0m val \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mwhere(mask)\n\u001b[0;32m---> 17\u001b[0m train\u001b[39m.\u001b[39;49mto_netcdf(train_dir\u001b[39m.\u001b[39;49mjoinpath(fp\u001b[39m.\u001b[39;49mname\u001b[39m.\u001b[39;49mreplace(\u001b[39m'\u001b[39;49m\u001b[39m.nc\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m.train.nc\u001b[39;49m\u001b[39m'\u001b[39;49m)))\n\u001b[1;32m     18\u001b[0m val\u001b[39m.\u001b[39mto_netcdf(val_dir\u001b[39m.\u001b[39mjoinpath(fp\u001b[39m.\u001b[39mname\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m.nc\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m.val.nc\u001b[39m\u001b[39m'\u001b[39m)))\n",
      "File \u001b[0;32m~/miniconda3/envs/spicy/lib/python3.11/site-packages/xarray/core/dataset.py:1911\u001b[0m, in \u001b[0;36mDataset.to_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1908\u001b[0m     encoding \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1909\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mxarray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackends\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m to_netcdf\n\u001b[0;32m-> 1911\u001b[0m \u001b[39mreturn\u001b[39;00m to_netcdf(  \u001b[39m# type: ignore  # mypy cannot resolve the overloads:(\u001b[39;49;00m\n\u001b[1;32m   1912\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1913\u001b[0m     path,\n\u001b[1;32m   1914\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   1915\u001b[0m     \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mformat\u001b[39;49m,\n\u001b[1;32m   1916\u001b[0m     group\u001b[39m=\u001b[39;49mgroup,\n\u001b[1;32m   1917\u001b[0m     engine\u001b[39m=\u001b[39;49mengine,\n\u001b[1;32m   1918\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   1919\u001b[0m     unlimited_dims\u001b[39m=\u001b[39;49munlimited_dims,\n\u001b[1;32m   1920\u001b[0m     compute\u001b[39m=\u001b[39;49mcompute,\n\u001b[1;32m   1921\u001b[0m     multifile\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1922\u001b[0m     invalid_netcdf\u001b[39m=\u001b[39;49minvalid_netcdf,\n\u001b[1;32m   1923\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/spicy/lib/python3.11/site-packages/xarray/backends/api.py:1217\u001b[0m, in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[39m# TODO: figure out how to refactor this logic (here and in save_mfdataset)\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[39m# to avoid this mess of conditionals\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1215\u001b[0m     \u001b[39m# TODO: allow this work (setting up the file for writing array data)\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m     \u001b[39m# to be parallelized with dask\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m     dump_to_store(\n\u001b[1;32m   1218\u001b[0m         dataset, store, writer, encoding\u001b[39m=\u001b[39;49mencoding, unlimited_dims\u001b[39m=\u001b[39;49munlimited_dims\n\u001b[1;32m   1219\u001b[0m     )\n\u001b[1;32m   1220\u001b[0m     \u001b[39mif\u001b[39;00m autoclose:\n\u001b[1;32m   1221\u001b[0m         store\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/spicy/lib/python3.11/site-packages/xarray/backends/api.py:1264\u001b[0m, in \u001b[0;36mdump_to_store\u001b[0;34m(dataset, store, writer, encoder, encoding, unlimited_dims)\u001b[0m\n\u001b[1;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m encoder:\n\u001b[1;32m   1262\u001b[0m     variables, attrs \u001b[39m=\u001b[39m encoder(variables, attrs)\n\u001b[0;32m-> 1264\u001b[0m store\u001b[39m.\u001b[39;49mstore(variables, attrs, check_encoding, writer, unlimited_dims\u001b[39m=\u001b[39;49munlimited_dims)\n",
      "File \u001b[0;32m~/miniconda3/envs/spicy/lib/python3.11/site-packages/xarray/backends/common.py:271\u001b[0m, in \u001b[0;36mAbstractWritableDataStore.store\u001b[0;34m(self, variables, attributes, check_encoding_set, writer, unlimited_dims)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_attributes(attributes)\n\u001b[1;32m    270\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_dimensions(variables, unlimited_dims\u001b[39m=\u001b[39munlimited_dims)\n\u001b[0;32m--> 271\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_variables(\n\u001b[1;32m    272\u001b[0m     variables, check_encoding_set, writer, unlimited_dims\u001b[39m=\u001b[39;49munlimited_dims\n\u001b[1;32m    273\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/spicy/lib/python3.11/site-packages/xarray/backends/common.py:313\u001b[0m, in \u001b[0;36mAbstractWritableDataStore.set_variables\u001b[0;34m(self, variables, check_encoding_set, writer, unlimited_dims)\u001b[0m\n\u001b[1;32m    308\u001b[0m check \u001b[39m=\u001b[39m vn \u001b[39min\u001b[39;00m check_encoding_set\n\u001b[1;32m    309\u001b[0m target, source \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_variable(\n\u001b[1;32m    310\u001b[0m     name, v, check, unlimited_dims\u001b[39m=\u001b[39munlimited_dims\n\u001b[1;32m    311\u001b[0m )\n\u001b[0;32m--> 313\u001b[0m writer\u001b[39m.\u001b[39;49madd(source, target)\n",
      "File \u001b[0;32m~/miniconda3/envs/spicy/lib/python3.11/site-packages/xarray/backends/common.py:162\u001b[0m, in \u001b[0;36mArrayWriter.add\u001b[0;34m(self, source, target, region)\u001b[0m\n\u001b[1;32m    160\u001b[0m     target[region] \u001b[39m=\u001b[39m source\n\u001b[1;32m    161\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m     target[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m] \u001b[39m=\u001b[39m source\n",
      "File \u001b[0;32m~/miniconda3/envs/spicy/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:68\u001b[0m, in \u001b[0;36mBaseNetCDF4Array.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__setitem__\u001b[39m(\u001b[39mself\u001b[39m, key, value):\n\u001b[0;32m---> 68\u001b[0m     \u001b[39mwith\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatastore\u001b[39m.\u001b[39;49mlock:\n\u001b[1;32m     69\u001b[0m         data \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_array(needs_lock\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     70\u001b[0m         data[key] \u001b[39m=\u001b[39;49m value\n",
      "File \u001b[0;32m~/miniconda3/envs/spicy/lib/python3.11/site-packages/xarray/backends/locks.py:165\u001b[0m, in \u001b[0;36mCombinedLock.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[39mfor\u001b[39;00m lock \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocks:\n\u001b[1;32m    163\u001b[0m         lock\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[0;32m--> 165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m    166\u001b[0m     \u001b[39mfor\u001b[39;00m lock \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocks:\n\u001b[1;32m    167\u001b[0m         lock\u001b[39m.\u001b[39m\u001b[39m__exit__\u001b[39m(\u001b[39m*\u001b[39margs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/56257429/randomly-mask-set-nan-x-of-data-points-in-huge-xarray-dataarray\n",
    "\n",
    "train_dir = Path('../../data/bootstrap/training')\n",
    "val_dir = Path('../../data/bootstrap/validation')\n",
    "\n",
    "train_dir.mkdir(parents = True, exist_ok = True)\n",
    "val_dir.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "for fp in Path('../../Lidar_s1_stacks').glob('*.nc'):\n",
    "    ds = xr.open_dataset(fp)\n",
    "    mask = xr.zeros_like(ds)['fcf'].rename('mask')\n",
    "    mask.data = np.random.rand(*mask.data.shape) < 0.2\n",
    "    mask = mask.broadcast_like(ds['s1'])\n",
    "    train = ds.where(~mask)\n",
    "    val = ds.where(mask)\n",
    "\n",
    "    train.to_netcdf(train_dir.joinpath(fp.name.replace('.nc','.train.nc')))\n",
    "    val.to_netcdf(val_dir.joinpath(fp.name.replace('.nc','.val.nc')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loop through .nc and resample and calculate all possible retrieved snowdepths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(0)\n",
    "# site_dir = Path('/Users/zachkeskinen/Documents/spicy-snow/scripts/optimize/param_sds/Mores_2021-03-15')\n",
    "# ds = xr.open_dataset(next(site_dir.glob('*')))\n",
    "# idx = ds['lidar-sd'].rename('mask')\n",
    "# idx.data = np.random.rand(*idx.data.shape) < 0.2\n",
    "# idx = idx.broadcast_like(ds)\n",
    "# for fp in site_dir.glob('*'):\n",
    "#     ds = xr.open_dataset(fp)\n",
    "#     a,b,c = fp.stem.split('_')\n",
    "#     train = ds.where(~idx)\n",
    "#     val = ds.where(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:48:25.321336  -- starting Mores_2020-02-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [06:33<00:00, 78.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:54:58.772782  -- starting Frasier_2021-03-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [07:18<00:00, 87.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:02:18.050228  -- starting Dry_Creek_2020-02-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [08:41<00:00, 104.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:11:00.581221  -- starting Banner_2021-03-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [10:31<00:00, 126.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:21:33.480180  -- starting Little_Cottonwood_2021-03-18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [08:30<00:00, 102.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:30:04.314760  -- starting Mores_2021-03-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [08:08<00:00, 97.66s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:38:13.481267  -- starting Banner_2020-02-18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [07:21<00:00, 88.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:45:35.957895  -- starting Frasier_2020-02-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [06:33<00:00, 78.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:52:10.153865  -- starting Cameron_2021-03-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [07:06<00:00, 85.21s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely import wkt\n",
    "from shapely.geometry import box\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from spicy_snow.processing.snow_index import calc_delta_cross_ratio, calc_delta_gamma, \\\n",
    "    clip_delta_gamma_outlier, calc_snow_index, calc_snow_index_to_snow_depth\n",
    "from spicy_snow.processing.wet_snow import id_newly_wet_snow, id_wet_negative_si, \\\n",
    "    id_newly_frozen_snow, flag_wet_snow\n",
    "\n",
    "# Create parameter space\n",
    "A = np.round(np.arange(1, 3.1, 0.5), 2)\n",
    "B = np.round(np.arange(0, 1.01, 0.1), 2)\n",
    "C = np.round(np.arange(0, 1.001, 0.01), 2)\n",
    "\n",
    "files = Path('../../Lidar_s1_stacks/').glob('*.nc')\n",
    "param_dir = Path('~/scratch/params').expanduser()\n",
    "for f in files:\n",
    "\n",
    "    # get dataset\n",
    "    ds_name = f.name.split('stacks/')[-1].split('.')[0]\n",
    "    print(datetime.now(), f' -- starting {ds_name}')\n",
    "    ds_ = xr.open_dataset(f).load()\n",
    "    dataset = ds_[['s1','deltaVV','ims','fcf','lidar-sd']]\n",
    "\n",
    "    # find closest timestep to lidar\n",
    "    td = abs(pd.to_datetime(dataset.time) - pd.to_datetime(dataset.attrs['lidar-flight-time']))\n",
    "    closest_ts = dataset.time[np.argmin(td)]\n",
    "\n",
    "    param_dir.joinpath(f'{ds_name}').mkdir()\n",
    "\n",
    "    # Brute-force processing loop\n",
    "    for a in tqdm(A):\n",
    "        # print(f'A: {a}')\n",
    "        ds = calc_delta_cross_ratio(dataset, A = a)\n",
    "        for b in B:\n",
    "            # print(f'    B: {b}')\n",
    "            ds = calc_delta_gamma(ds, B = b, inplace=False)\n",
    "            ds = clip_delta_gamma_outlier(ds)\n",
    "            ds = calc_snow_index(ds)\n",
    "            ds = id_newly_wet_snow(ds)\n",
    "            ds = id_wet_negative_si(ds)\n",
    "            ds = id_newly_frozen_snow(ds)\n",
    "            ds = flag_wet_snow(ds)\n",
    "            for c in C:\n",
    "                # print(f'        c: {c}')\n",
    "                # print(f'A={a}; B={b}; C={c}')\n",
    "\n",
    "                ds = calc_snow_index_to_snow_depth(ds, C = c)\n",
    "\n",
    "                sub = ds.sel(time = closest_ts)[['snow_depth', 'wet_snow', 'lidar-sd']]\n",
    "                sub.to_netcdf(param_dir.joinpath(f'{ds_name}/{a}_{b}_{c}.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def get_bootstrap(x, y):\n",
    "    # ravel to numpy arrays\n",
    "    x = x.values.ravel()\n",
    "    y = y.values.ravel()\n",
    "\n",
    "    # remove nans\n",
    "    x_buff = x[(~np.isnan(x)) & (~np.isnan(y))]\n",
    "    y = y[(~np.isnan(x)) & (~np.isnan(y))]\n",
    "    x = x_buff\n",
    "\n",
    "    # bootstrap\n",
    "    x_bs = np.random.choice(x, size = len(x))\n",
    "    y_bs = np.random.choice(y, size = len(y))\n",
    "\n",
    "    return x_bs, y_bs\n",
    "\n",
    "def calc_rmse(y_actual, y_pred):\n",
    "    rms = mean_squared_error(y_actual, y_pred, squared = False)\n",
    "    return rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bsuhome/zacharykeskinen/miniconda3/envs/spicy/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 5555/5555 [00:00<00:00, 6797.72it/s]\n",
      "100%|██████████| 5555/5555 [00:00<00:00, 6829.80it/s]\n",
      "100%|██████████| 5555/5555 [00:00<00:00, 6855.89it/s]\n",
      "100%|██████████| 5555/5555 [00:00<00:00, 6823.18it/s]\n",
      "100%|██████████| 5555/5555 [00:00<00:00, 6855.44it/s]\n",
      "100%|██████████| 5555/5555 [00:00<00:00, 6817.25it/s]\n",
      "100%|██████████| 5555/5555 [00:00<00:00, 6813.11it/s]\n",
      "100%|██████████| 5555/5555 [00:00<00:00, 6711.08it/s]\n",
      "100%|██████████| 5555/5555 [00:00<00:00, 6734.02it/s]\n",
      "100%|██████████| 5555/5555 [00:00<00:00, 6773.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# from itertools import product\n",
    "from tqdm.contrib.itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create parameter space\n",
    "A = np.round(np.arange(1, 3.1, 0.5), 2)\n",
    "B = np.round(np.arange(0, 1.01, 0.1), 2)\n",
    "C = np.round(np.arange(0, 1.001, 0.01), 2)\n",
    "ABC = [A, B, C]\n",
    "\n",
    "param_dir = Path('~/scratch/params').expanduser()\n",
    "\n",
    "df = pd.DataFrame(np.empty((10, 4), dtype = float), columns = ['a', 'b', 'c', 'rmse'])\n",
    "\n",
    "for i in range(10):\n",
    "    np.random.seed(i)\n",
    "\n",
    "    rmse_no_flag = xr.DataArray(np.empty((len(A), len(B), len(C)))*np.nan,\n",
    "                            coords=(A, B, C), dims=('A','B','C'))\n",
    "\n",
    "    for a, b, c in product(*ABC):\n",
    "        # lidar = []\n",
    "        # spicy = []\n",
    "        # for loc_dir in param_dir.glob('*'):\n",
    "        #     res = xr.open_dataset(param_dir.joinpath(f'{loc_dir.name}/{a}_{b}_{c}.nc'))\n",
    "        #     sd_actual, sd_pred = get_bootstrap(res['lidar-sd'], res['snow_depth'])\n",
    "        #     lidar.extend(sd_actual)\n",
    "        #     spicy.extend(sd_pred)\n",
    "            \n",
    "        rmse_no_flag.loc[a,b,c] = np.random.random() #calc_rmse(lidar, spicy)\n",
    "\n",
    "    best = rmse_no_flag.where(rmse_no_flag==rmse_no_flag.min(), drop=True).squeeze()\n",
    "    a, b, c = best.coords.values()\n",
    "    df.loc[i, 'a'] = a\n",
    "    df.loc[i, 'b'] = b\n",
    "    df.loc[i, 'c'] = c\n",
    "    df.loc[i, 'rmse'] = best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/bsuhome/zacharykeskinen/spicy-snow/scripts/optimize/param_pdf/pdf_v1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spicy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
